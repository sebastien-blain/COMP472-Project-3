{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac7802e",
   "metadata": {},
   "source": [
    "# Task 2: Comparison with Other Pre-trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453a9b56",
   "metadata": {},
   "source": [
    "Now that you have obtained results with the word2vec-google-news-300 pre-trained model, you will experi-\n",
    "ment with 4 other English word2vec pretrained models and compare the results. You can use any pre-trained\n",
    "embeddings that you want, but you must have:\n",
    "\n",
    "1. 2 new models from different corpora (eg. Twitter, English Wikipedia Dump . . . ) but same embedding size\n",
    "(eg. 25, 100, 300)\n",
    "\n",
    "2. 2 new models from the same corpus but different embedding sizes\n",
    "\n",
    "Many pre-trained embeddings are available on line (including in Gensim or at http://vectors.nlpl.eu/repository).\n",
    "For each model that you use, create a new <model name>-details.csv output file and append the results\n",
    "to the file analysis.csv (see Section 2.2). For example, the file analysis.csv could now contain:\n",
    "\n",
    "word2vec-google-news-300,3000000,44,78,0.5641025641025641 // from Task 1  \n",
    "C1-E1,...,...,...,...  \n",
    "C2-E2,...,...,...,...  \n",
    "C3-E3,...,...,...,...  \n",
    "C4-E4,...,...,...,...  \n",
    "where C1 to C4 refer to the corpora and E1 to E4 refer to their embedding sizes, and C1 ̸= C2 and E1 = E2\n",
    "and C3 = C4 and E3 ̸= E4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cd59ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import downloader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6576799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the 4 different models\n",
    "model1 = 'glove-twitter-100'\n",
    "model2 = 'glove-wiki-gigaword-100'\n",
    "model3 = 'glove-twitter-25'\n",
    "model4 = 'glove-twitter-50'\n",
    "model_names = [model1, model2,model3, model4 ]\n",
    "models = {}\n",
    "\n",
    "for model in model_names:\n",
    "    models[model] = downloader.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c977609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of        question        answer              0               1              2  \\\n",
      "0    enormously  tremendously  appropriately        uniquely   tremendously   \n",
      "1    provisions  stipulations   stipulations  interrelations  jurisdictions   \n",
      "2   haphazardly      randomly    dangerously         densely       randomly   \n",
      "3     prominent   conspicuous       battered         ancient     mysterious   \n",
      "4        zenith      pinnacle     completion        pinnacle         outset   \n",
      "..          ...           ...            ...             ...            ...   \n",
      "75      fashion        manner         ration          fathom          craze   \n",
      "76     marketed          sold         frozen            sold      sweetened   \n",
      "77       bigger        larger       steadier          closer         larger   \n",
      "78        roots       origins        origins         rituals           cure   \n",
      "79     normally    ordinarily      haltingly      ordinarily    permanently   \n",
      "\n",
      "                  3  \n",
      "0         decidedly  \n",
      "1   interpretations  \n",
      "2          linearly  \n",
      "3       conspicuous  \n",
      "4           decline  \n",
      "..              ...  \n",
      "75           manner  \n",
      "76          diluted  \n",
      "77           better  \n",
      "78         function  \n",
      "79     periodically  \n",
      "\n",
      "[80 rows x 6 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Load the file\n",
    "df = pd.read_csv('synonyms.csv')\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "351d0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output files\n",
    "for model in model_names:\n",
    "    fs = open('{}-details.csv'.format(model), 'w')\n",
    "    fs.write('question,answer,guess,label\\n')\n",
    "    fs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f5bbd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_names:\n",
    "    fs = open('{}-details.csv'.format(model), 'a')\n",
    "\n",
    "    CORRECT_LABEL = 0\n",
    "    ANSWERED_QUESTIONS = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        guesses = [row['0'], row['1'], row['2'], row['3']]\n",
    "        best_guess = (0, '')\n",
    "        for guess in guesses:\n",
    "            try:\n",
    "                sim = models[model].similarity(question, guess)\n",
    "                # Check if the similarity is greater than the current best guess\n",
    "                if sim > best_guess[0]:\n",
    "                    best_guess = (sim, guess)\n",
    "            except:\n",
    "                # Could be because bhe question isn't in the model\n",
    "                pass\n",
    "\n",
    "        if best_guess[1] != '':\n",
    "            ANSWERED_QUESTIONS += 1\n",
    "            # If the guess is correct\n",
    "            if best_guess[1] == answer:\n",
    "                label = 'correct'\n",
    "                CORRECT_LABEL += 1\n",
    "            # If the guess is wrong\n",
    "            else:\n",
    "                label = 'wrong'\n",
    "        else:\n",
    "            label = 'guess'\n",
    "\n",
    "\n",
    "        fs.write(','.join([question, answer, best_guess[1], label]) + '\\n')\n",
    "\n",
    "    fs.close()\n",
    "    \n",
    "    # analysis part\n",
    "    fs = open('analysis.csv', 'a')\n",
    "\n",
    "    model_name = model\n",
    "    size_of_vocabulary = len(models[model])\n",
    "    correct_label = CORRECT_LABEL\n",
    "    answered_questions = ANSWERED_QUESTIONS\n",
    "    accuracy = correct_label / answered_questions\n",
    "\n",
    "    # Write the analysis to file: model_name, size of voc, correct label, answered questions, accuracy\n",
    "    fs.write('\\n' + ','.join([model_name, str(size_of_vocabulary), str(correct_label), str(answered_questions), str(accuracy)]))\n",
    "\n",
    "    fs.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
